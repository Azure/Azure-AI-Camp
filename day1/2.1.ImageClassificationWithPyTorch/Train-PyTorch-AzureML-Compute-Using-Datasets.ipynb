{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a PyTorch Classification Model with Azure ML and AML Compute\n",
    "## Using zip file and Azure ML Dataset as data source\n",
    "\n",
    "\n",
    "Here we have a driver notebook that uses Azure ML Python SDK to create AmlCompute (compute cluster for training) and a PyTorch estimator to tell Azure ML where to find the right resources and how to train.\n",
    "\n",
    "Note:\n",
    "* Please use the \"Python 3.6 - Azure ML\" kernel on the DSVM for this notebook or install appropriate library versions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Install/upgrade the Azure ML SDK using: \n",
    "# pip and the correct Python kernel with sys.executable\n",
    "# ! {sys.executable} -m pip install --upgrade azureml-sdk[notebooks]==1.2.0\n",
    "# ! {sys.prefix}/bin/pip install matplotlib\n",
    "\n",
    "## If running locally or need to upgrade PyTorch - install PyTorch and Torchvision with\n",
    "! {sys.prefix}/bin/pip install --upgrade torch==1.2 torchvision==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Model\n",
    "from azureml.exceptions import ProjectSystemException\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Dataset\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "import torch\n",
    "\n",
    "print(\"SDK version: \", azureml.core.VERSION)\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your experiment a prefix\n",
    "my_nickname = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for your cluster - under 16 characters\n",
    "cluster_name = \"gpuforpytorch\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # AML Compute config - if max_nodes are set, it becomes persistent storage that scales\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                        min_nodes=0,\n",
    "                                                        max_nodes=3)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the provisioning status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project directory and copy training script to it\n",
    "project_folder = os.path.join(os.getcwd(), 'project')\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy(os.path.join(os.getcwd(), 'pytorch_train_transfer_dataset.py'), project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment\n",
    "\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial.\n",
    "\n",
    "Think of an experiment like a scenario such as \"finding images of people fighting in CCTV feeds\".  An experiment usually will have many \"runs\" which could entail updates to the data, hyperparameters, training code itself, and other optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "experiment_name = 'suspicious-behavior-'+my_nickname\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset\n",
    "\n",
    "The data source is a subset of ImageNet.  It can be downloaded by clicking:  https://download.pytorch.org/tutorial/hymenoptera_data.zip.  The following steps set up the Dataset from the default data store in the Workspace and register it so that scripts and compute can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = 'behavior_'+my_nickname\n",
    "\n",
    "# Get the datastore to upload prepared data\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Create a File Dataset from 1 URL path\n",
    "url_path = ['https://github.com/harris-soh-copeland-puca/SampleFiles/raw/master/caviar_small.zip']\n",
    "behavior_ds = Dataset.File.from_files(path=url_path)\n",
    "\n",
    "# Register the dataset so that scripts and compute may access\n",
    "behavior_ds = behavior_ds.register(workspace=ws,\n",
    "                                 name='behavior_ds_'+my_nickname,\n",
    "                                 description='Subset of CAVIAR dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "To train the PyTorch model we are going to use a Azure ML Estimator specific to PyTorch - see [Train models with Azure Machine Learning using estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models) for more on Estimators.  We will use the Datastore we specified earlier which mounts the Blob Storage container to the remote compute target for training in this case.\n",
    "\n",
    "To learn more about where read and write files in a local or remote compute see [Where to save and write files for Azure Machine Learning experiments](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-save-write-experiment-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for training (\"trans\" flag means - use transfer learning and \n",
    "# this should download a model on compute)\n",
    "# Using /tmp to store model and info due to the fact that\n",
    "# creating new folders and files on the Azure Function host\n",
    "# will trigger the function to restart.\n",
    "script_params = {\n",
    "    '--data_dir': behavior_ds.as_named_input('behavior_ds_'+my_nickname).as_mount(),\n",
    "    '--num_epochs': 30,\n",
    "    '--learning_rate': 0.01,\n",
    "    '--output_dir': './outputs',\n",
    "    '--trans': 'True'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PyTorch estimator with upload of final model to\n",
    "# a specified blob storage container (this can be anything)\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='pytorch_train_transfer_dataset.py',\n",
    "                    use_gpu=True,\n",
    "                    pip_packages=['matplotlib==3.1.1',\n",
    "                                  'opencv-python==4.1.1.26', \n",
    "                                  'Pillow==6.2.1'],\n",
    "                   framework_version='1.3')\n",
    "\n",
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check run status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_details()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check run status, interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model to workspace\n",
    "\n",
    "This will allow accessibility to the model through the SDK in other runs or experiments.\n",
    "\n",
    "This code is found in the training script where access exists to the run object.\n",
    "\n",
    "```python\n",
    "model = run.register_model(model_name='pt-dnn', model_path='outputs/model_finetuned.pth')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively, register within this notebook \n",
    "# (the model_path is the Azure ML workspace model path, not local)\n",
    "\n",
    "## Get one particular Run using run id found in Azure Portal\n",
    "# from azureml.core import Run\n",
    "# run = Run(experiment, run_id='suspicious-behavior-...')\n",
    "\n",
    "# Register model to Models in workspace\n",
    "model = run.register_model(model_name='behavior-pytorch-'+my_nickname, model_path='outputs/model_finetuned.pth',\n",
    "                          description='Squeezenet PyTorch model; 30 epochs; 0.01 LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "You will need test images in the test_images folder in the following folder structure:\n",
    "\n",
    "```\n",
    "data\n",
    "    \\test\n",
    "        \\normal\n",
    "        \\suspicious\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"data\" folder is in the current working directory\n",
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model( ws, 'behavior-pytorch-'+my_nickname, version=1).download(exist_ok=True)\n",
    "model = torch.load('model_finetuned.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch datasets and dataloaders.  Using PyTorch's `ImageFolder` to read a folder of images for classification.  It uses the names of the folders as class names.  Here, reading the `test` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, 'test_images', x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1,\n",
    "                                              shuffle=False, num_workers=0)\n",
    "               for x in ['test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['test']}\n",
    "class_names = image_datasets['test'].classes\n",
    "print(dataset_sizes['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peform inference on test data set to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over data.\n",
    "running_corrects = 0\n",
    "for inputs, labels in dataloaders['test']:\n",
    "\n",
    "    # Don't need to track history in the tensors\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "    # Statistics\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "overall_acc = running_corrects.double() / dataset_sizes['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy = ', overall_acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
