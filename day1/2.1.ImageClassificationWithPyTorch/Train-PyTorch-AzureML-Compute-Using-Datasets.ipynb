{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a PyTorch Classification Model with Azure ML and AML Compute\n",
    "## Using zip file and Azure ML Dataset as data source\n",
    "\n",
    "\n",
    "Here we have a driver notebook that uses Azure ML Python SDK to create AmlCompute (compute cluster for training) and a PyTorch estimator to tell Azure ML where to find the right resources and how to train.  The dataset used here for image classification with deep learning is a small subset of the <a href=\"https://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/\" target=\"_blank\">CAVIAR dataset</a>.  For better accuracy with this process it is recommended to use a larger dataset, however for demo purposes, the dataset used here is small.\n",
    "\n",
    "IMPORTANT:\n",
    "* Please use the **\"Python 3.6 - Azure ML\" kernel** on the DSVM for this notebook or install appropriate library versions below (to change a kernel go to Kernel in the menu bar and select \"Change kernel\").\n",
    "* You will need your `config.json` from your Azure ML Workspace in the **same folder** as this notebook and interactive login will be performed later on so be prepared with your Azure login info.  You may wish to work with these notebooks in an **Incognito or Private browser window** in case you have other Azure accounts.\n",
    "* See `alternative` folder for the alternative way of connecting to data with a DataStore wrapping Azure Blob Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip1:  To ensure that the packages are installed into the correct kernel for a notebook we use `{sys.prefix}` (or `{sys.executable} -m`, alternatively) for the path to the correct Python environment.\n",
    "<br><br>\n",
    "Tip 2:  It's a good idea to make sure the releases of `torch` and `torchvision` match (you can check on https://pypi.org/ for the release dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set warnings and logging level\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "# Install packages\n",
    "import sys\n",
    "\n",
    "# Install/upgrade the Azure ML SDK using: \n",
    "# pip and the correct Python kernel with sys.executable\n",
    "! {sys.executable} -m pip install -q --upgrade azureml-sdk[notebooks,automl,contrib]==1.5.0\n",
    "# ! {sys.prefix}/bin/pip install matplotlib\n",
    "\n",
    "## If running locally or need to upgrade PyTorch - install PyTorch and Torchvision with\n",
    "! {sys.prefix}/bin/pip install -q --upgrade torch==1.3 torchvision==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Model\n",
    "from azureml.exceptions import ProjectSystemException\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Dataset\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "import torch\n",
    "\n",
    "print(\"SDK version: \", azureml.core.VERSION)\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your experiment a suffix\n",
    "my_nickname = ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`.  This will trigger an interactive login so please follow the instructions after running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, we use Azure ML managed compute ([AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)) for our remote training compute resource.\n",
    "\n",
    "**Creation of AmlCompute could take several minutes.** If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for your cluster - under 16 characters\n",
    "cluster_name = \"gpuforpytorch\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # AML Compute config - if max_nodes are set, it becomes persistent/dedicated resource that scales\n",
    "    # Set min_nodes to 0 so that it scales down to 0 VMs when not is use to avoid incurring costs\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                        min_nodes=0,\n",
    "                                                        max_nodes=1)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the provisioning status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the project directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location here, where we are running this notebook, can be thought of as a local machine.  The compute target just created is where the training will actually take place.  Here, we create a `project` folder.  All of the contents of this folder are special in that they are uploaded to the target compute when we begin a run of this experiment so we definitely want our training script uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project directory and copy training script to it\n",
    "project_folder = os.path.join(os.getcwd(), 'project')\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "shutil.copy(os.path.join(os.getcwd(), 'pytorch_train_transfer_dataset.py'), project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment\n",
    "\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this transfer learning PyTorch tutorial.\n",
    "\n",
    "Think of an experiment like a scenario such as \"finding images of people fighting in CCTV feeds\".  An experiment usually will have many \"runs\" which could entail updates to the data, hyperparameters, training code itself, and other optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "experiment_name = 'suspicious-behavior-'+my_nickname\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset\n",
    "\n",
    "The data source is a subset of ImageNet.  It can be downloaded by clicking:  https://download.pytorch.org/tutorial/hymenoptera_data.zip.  The following steps set up the Dataset from the default data store in the Workspace and register it so that scripts and compute can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a File Dataset from 1 URL path\n",
    "url_path = ['https://github.com/harris-soh-copeland-puca/SampleFiles/raw/master/caviar_small.zip']\n",
    "behavior_ds = Dataset.File.from_files(path=url_path)\n",
    "\n",
    "# Register the dataset so that scripts and compute may access\n",
    "behavior_ds = behavior_ds.register(workspace=ws,\n",
    "                                 name='behavior_ds_'+my_nickname,\n",
    "                                 description='Subset of CAVIAR dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "To train the PyTorch model we are going to use a Azure ML Estimator specific to PyTorch - see [Train models with Azure Machine Learning using estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models) for more on Estimators.  We will use the Datastore we specified earlier which mounts the Blob Storage container to the remote compute target for training in this case.\n",
    "\n",
    "To learn more about where read and write files in a local or remote compute see [Where to save and write files for Azure Machine Learning experiments](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-save-write-experiment-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for training (\"transfer\" flag means - use transfer learning and \n",
    "# this should download a model on target compute)\n",
    "script_params = {\n",
    "    '--data_dir': behavior_ds.as_named_input('behavior_ds_'+my_nickname).as_mount(),\n",
    "    '--num_epochs': 10,\n",
    "    '--learning_rate': 0.01,\n",
    "    '--output_dir': './outputs',\n",
    "    '--transfer': 'True'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PyTorch estimator with upload of final model to\n",
    "# a specified blob storage container (this can be anything)\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='pytorch_train_transfer_dataset.py',\n",
    "                    use_gpu=True,\n",
    "                    pip_packages=['matplotlib==3.1.1',\n",
    "                                  'opencv-python==4.1.1.26', \n",
    "                                  'Pillow==6.2.1'],\n",
    "                   framework_version='1.3')\n",
    "\n",
    "# Submit experiment run to Azure ML\n",
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check run status.  You can re-run this cell to recheck.  Training with the data provided through this notebook \"as-is\" will take ~30-40 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_details()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check run status, interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the trained model to workspace\n",
    "\n",
    "This will allow accessibility to the model through the SDK in other runs or experiments.  It will also be available for download.\n",
    "\n",
    "You could even have registered the model in the training script directly:\n",
    "\n",
    "```python\n",
    "model = run.register_model(model_name='pt-dnn', model_path='outputs/model_finetuned.pth')\n",
    "```\n",
    "\n",
    "Below, we register from this notebook.  After running, check the Azure Portal that the model does indeed appear registered to the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively, register within this notebook \n",
    "# (the model_path is the Azure ML workspace model path, not local)\n",
    "\n",
    "## Get one particular Run using run id found in Azure Portal\n",
    "# from azureml.core import Run\n",
    "# run = Run(experiment, run_id='suspicious-behavior-...')\n",
    "\n",
    "# Register model to Models in workspace\n",
    "model = run.register_model(model_name='behavior-pytorch-'+my_nickname, model_path='outputs/model_finetuned.pth',\n",
    "                          description='Squeezenet PyTorch model; 30 epochs; 0.01 LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "- Get the test image (file:  `caviar_test_images.zip`) from the Release page for this repo on GitHub (https://github.com/Azure/Azure-AI-Camp/releases).\n",
    "- Upload to the `Azure-AI-Camp/day1/2.1.ImageClassificationWithPyTorch/` folder\n",
    "- Unzip the file `unzip caviar_test_images.zip` on the command line (go to \"New\" and select \"Terminal\" from drop-down in Jupyterhub)\n",
    "\n",
    "You will need test images in a folder named `test` with the following folder structure:\n",
    "\n",
    "```\n",
    "\\test\n",
    "    \\normal\n",
    "    \\suspicious\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"data\" folder is in the current working directory\n",
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model from Azure ML Workspace.  Note, you can specify a `version` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(ws, 'behavior-pytorch-'+my_nickname, version=1).download(exist_ok=True)\n",
    "model = torch.load('model_finetuned.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transforms with `torchvision` (similar to transforms during training minus any data augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch datasets and dataloaders.  Using PyTorch's `ImageFolder` to read a folder of images for classification.  It uses the names of the folders as class names.  Here, reading the `test` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1,\n",
    "                                              shuffle=False, num_workers=0)\n",
    "               for x in ['test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['test']}\n",
    "class_names = image_datasets['test'].classes\n",
    "print(dataset_sizes['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peform inference on test data set to evaluate.  This may take a few minutes, depending upon your CPU compute and number of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over data.\n",
    "running_corrects = 0\n",
    "for inputs, labels in dataloaders['test']:\n",
    "\n",
    "    # Don't need to track history in the tensors\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "    # Statistics\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "overall_acc = running_corrects.double() / dataset_sizes['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using the small CAVIAR dataset, the accuracy will be fairly low (~50%).  Try the experiment again with more data or a different dataset, also tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy = ', overall_acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Update your training experiment to use hyperparameter tuning and `hyperdrive` to discover the best learning rate and number of epochs.\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload files to a Datastore and create a Dataset with the SDK\n",
    "\n",
    "Given there is a folder called `test` with sets of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core import Dataset\n",
    "import glob\n",
    "\n",
    "ws = Workspace.from_config(path='config.json')\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "# Uploads a folder of files to a folder path in the default Azure ML Blob storage\n",
    "datastore.upload_files(files=glob.glob('./test/**/*.*', recursive=True),\n",
    "                       target_path='caviar-small-testset/',\n",
    "                       overwrite=False,\n",
    "                       show_progress=False)\n",
    "\n",
    "# Instantiates a Dataset to use in training, etc.\n",
    "dataset = Dataset.File.from_files(path = [(datastore, 'caviar-small-testset/')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
